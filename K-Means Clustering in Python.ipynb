{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://stamfordresearch.com/k-means-clustering-in-python/ \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=Lm1c2U8BmoA (PySpark, ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set Matplotlib inline plotting and load Pandas package\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "data = hive_ctx.sql(\"Select * from bi_temp_kmeanClusteringtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 results\n",
    "\n",
    "df = data.toPandas()\n",
    "\n",
    "df.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe().transpose().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Nas by mean of column for firstsessionduration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df['firstsessionduration'].fillna(df['firstsessionduration'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Nas by 0 for other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only \n",
    "\n",
    "cols = df.columns[(df.dtypes == 'int64') | (df.dtypes == 'float64')] # '|' = or \n",
    "\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[cols].head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New DataFrame without the s__uid \n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix of covariance\n",
    "\n",
    "sampled_data = df[cols].sample(frac=0.1) \n",
    "\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(12, 12)); \n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other type of visuzalization\n",
    "\n",
    "# Source: http://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title('Feature Correlation')\n",
    "    labels= df.columns # not sure\n",
    "    ax1.set_xticklabels(labels,fontsize=6)\n",
    "    ax1.set_yticklabels(labels,fontsize=6)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    cbar = fig.colorbar(cax, ticks=[.25,.3,.35,.4,.45,.5,.55,.6,.65,.70,.75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "    \n",
    "correlation_matrix(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Standardize Features\n",
    " \n",
    " source: http://stackoverflow.com/questions/12525722/normalize-data-in-pandas \n",
    " \n",
    "\"In cluster analysis variables with large values contribute more to the distance calculations. Variables measured on different scales should be standardized prior to clustering, so that the solution is not driven by variables measured on larger scales.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standardize the data attributes\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(np_scaled)\n",
    "\n",
    "df_normalized.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set column names back\n",
    "\n",
    "df_normalized.columns = cols\n",
    "\n",
    "df_normalized.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection \n",
    "\n",
    "- RandomForrest\n",
    "- Lasso\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mean clustering using Spark ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_normalized.dtypes\n",
    "\n",
    "type(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_normalized = sqlContext.createDataFrame(df_normalized)\n",
    "\n",
    "'''\n",
    "vectorAssembler = VectorAssembler(inputCols= df_normalized.columns,\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df = vectorAssembler.transform(df_normalized)\n",
    "'''\n",
    "\n",
    "rdd = df_normalized.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "\n",
    "clusters = KMeans.train(rdd, 5, maxIterations=10, initializationMode=\"random\") # 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class RowIterator(TransformerMixin):\n",
    "    \"\"\" Prepare dataframe for DictVectorizer \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (row for _, row in X.iterrows())\n",
    "\n",
    "\n",
    "vectorizer = make_pipeline(RowIterator(), DictVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the sum of Squared Error:\n",
    "\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Within-cluster sum of squares\n",
    "\n",
    "WSSE = (rdd.map(lambda point: error(point))\n",
    "                   .reduce(lambda x,y: x+y))\n",
    "\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try with a range of number of clusters\n",
    "\n",
    "for l in range(1,6):\n",
    "    clusters = KMeans.train(rdd, l, maxIterations = 100, runs = 100, initializationMode = 'random')\n",
    "    WSSSE = (rdd.map(lambda point: error(point))\n",
    "                .reduce(lambda x,y: x+y))\n",
    "    print(\"With \" + str(l) + ' clusters: Within Set Sum of Squared Error =' + str(WSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-mean clustering using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MasterClass: https://github.com/Marie-de-Leseleuc/Python-Code/blob/master/exo%2B1%20(2).ipynb \n",
    "\n",
    "1. https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials \n",
    "\n",
    "2. https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\n",
    "\n",
    "3. http://mnemstudio.org/clustering-k-means-example-1.htm\n",
    "\n",
    "4. https://www.dataquest.io/blog/k-means-clustering-us-senators/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: http://stackoverflow.com/questions/28017091/will-pandas-dataframe-object-work-with-sklearn-kmeans-clustering \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "dataset = df_normalized\n",
    "\n",
    "# Convert DataFrame to matrix\n",
    "mat = dataset.as_matrix()\n",
    "\n",
    "# Using sklearn\n",
    "km = KMeans(n_clusters=5)\n",
    "km.fit(mat)\n",
    "\n",
    "# Get cluster assignment labels\n",
    "labels = km.labels_\n",
    "\n",
    "# Format results as a DataFrame\n",
    "results = pd.DataFrame([dataset.index,labels]).T # return a df with the cluster corresponding to each index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add cluster number to  \n",
    "\n",
    "print(len(dataset), len(results))\n",
    "\n",
    "dataset_f = dataset\n",
    "\n",
    "dataset_f['cluster'] = results[1] # Add cluster number to df\n",
    "\n",
    "dataset_f['s__uid'] = data.toPandas()['s__uid'] # add s__uid to df\n",
    "\n",
    "dataset_f[['cluster', 's__uid']].head(5) # produce cluster by player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the clusters\n",
    "\n",
    "labels = dataset_f.cluster.unique()\n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    print(dataset_f[dataset_f[\"cluster\"]==label].describe())\n",
    "    dataset_f[dataset_f[\"cluster\"]==label].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.drop('s__uid', axis=1, inplace=True)\n",
    "\n",
    "dataset.drop('cluster', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: http://stackoverflow.com/questions/34958994/how-to-use-scikit-kmeans-when-i-have-a-dataframe\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "sample_df_train, sample_df_test = sklearn.cross_validation.train_test_split(dataset, train_size=0.6)\n",
    "\n",
    "cluster = sklearn.cluster.KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=300, tol=0.0001, \n",
    "                                 precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1)\n",
    "\n",
    "cluster.fit(sample_df_train)\n",
    "\n",
    "result = cluster.predict(sample_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result)\n",
    "\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: could have join the two table using the index as key (function Merge(on=)). \n",
    "\n",
    "dataset_f2 = dataset\n",
    "\n",
    "dataset_f2['cluster'] = result[0] # Add cluster number to df\n",
    "\n",
    "dataset_f2['s__uid'] = data.toPandas()['s__uid'] # add s__uid to df\n",
    "\n",
    "dataset_f2[['cluster', 's__uid']].head(5) # produce cluster by player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the clusters\n",
    "\n",
    "labels = dataset_f2.cluster.unique()\n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    print(dataset_f2[dataset_f2[\"cluster\"]==label].describe())\n",
    "    dataset_f2[dataset_f2[\"cluster\"]==label].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_f2.cluster.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.drop('s__uid', axis=1, inplace=True)\n",
    "\n",
    "dataset.drop('cluster', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Source: \n",
    "#https://www.coursera.org/learn/machine-learning-data-analysis/lecture/Ebb2M/running-a-k-means-cluster-analysis-in-python-pt-1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# split data into train and test sets\n",
    "clus_train, clus_test = train_test_split(dataset, test_size=.3, random_state=123)\n",
    "\n",
    "# k-means cluster analysis for 1-9 clusters                                                           \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,10)\n",
    "meandist=[] #store distance values from the cluster centroids\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)  # specify number of clusters to use for the analysis\n",
    "    model.fit(clus_train)  # cluster analysis\n",
    "    clusassign=model.predict(clus_train) # cluster number that is assigned to each obs. based on the cluster analysis\n",
    "    meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, 'euclidean'), axis=1)) # computes the average of \n",
    "                                            # the sum of the distances between each observation in the cluster centroids\n",
    "    / clus_train.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(clusters, meandist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpret 3 cluster solution\n",
    "model3=KMeans(n_clusters=3)\n",
    "model3.fit(clus_train)\n",
    "clusassign=model3.predict(clus_train)\n",
    "# plot clusters\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca_2 = PCA(2)\n",
    "plot_columns = pca_2.fit_transform(clus_train)\n",
    "plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=model3.labels_,)\n",
    "plt.xlabel('Canonical variable 1')\n",
    "plt.ylabel('Canonical variable 2')\n",
    "plt.title('Scatterplot of Canonical Variables for 3 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BEGIN multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster training data to merge with the cluster assignment variable\n",
    "clus_train.reset_index(level=0, inplace=True)\n",
    "# create a list that has the new index variable\n",
    "cluslist=list(clus_train['index'])\n",
    "# create a list of cluster assignments\n",
    "labels=list(model3.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine index variable list with cluster assignment list into a dictionary\n",
    "newlist=dict(zip(cluslist, labels))\n",
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert newlist dictionary to a dataframe\n",
    "newclus= pd.DataFrame.from_dict(newlist, orient='index')\n",
    "newclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename the cluster assignment column\n",
    "newclus.columns = ['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now do the same for the cluster assignment variable\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster assignment dataframe \n",
    "# to merge with cluster training data\n",
    "newclus.reset_index(level=0, inplace=True)\n",
    "# merge the cluster assignment dataframe with the cluster training variable dataframe\n",
    "# by the index variable\n",
    "merged_train=pd.merge(clus_train, newclus, on='index')\n",
    "merged_train.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster frequencies\n",
    "merged_train.cluster.value_counts()\n",
    "\n",
    "\"\"\"\n",
    "END multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FINALLY calculate clustering variable means by cluster\n",
    "clustergrp = merged_train.groupby('cluster').mean()\n",
    "print (\"Clustering variable means by cluster\")\n",
    "print(clustergrp)\n",
    "\n",
    "# validate clusters in training data by examining cluster differences in GPA using ANOVA\n",
    "# first have to merge GPA with clustering variables and cluster assignment data \n",
    "gpa_data=data_clean['GPA1']\n",
    "# split GPA data into train and test sets\n",
    "gpa_train, gpa_test = train_test_split(gpa_data, test_size=.3, random_state=123)\n",
    "gpa_train1=pd.DataFrame(gpa_train)\n",
    "gpa_train1.reset_index(level=0, inplace=True)\n",
    "merged_train_all=pd.merge(gpa_train1, merged_train, on='index')\n",
    "sub1 = merged_train_all[['GPA1', 'cluster']].dropna()\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi \n",
    "\n",
    "gpamod = smf.ols(formula='GPA1 ~ C(cluster)', data=sub1).fit()\n",
    "print (gpamod.summary())\n",
    "\n",
    "print ('means for GPA by cluster')\n",
    "m1= sub1.groupby('cluster').mean()\n",
    "print (m1)\n",
    "\n",
    "print ('standard deviations for GPA by cluster')\n",
    "m2= sub1.groupby('cluster').std()\n",
    "print (m2)\n",
    "\n",
    "mc1 = multi.MultiComparison(sub1['GPA1'], sub1['cluster'])\n",
    "res1 = mc1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
