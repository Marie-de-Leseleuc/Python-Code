{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Prediction with PySpark using MLlib and ML Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check version of python\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: https://www.mapr.com/blog/churn-prediction-pyspark-using-mllib-and-ml-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set Matplotlib inline plotting and load Pandas package\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, HiveContext, Row\n",
    "\n",
    "hive_ctx = HiveContext(sc)\n",
    "hive_ctx.setConf(\"hive.exec.dynamic.partition\", \"true\")\n",
    "hive_ctx.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "hive_ctx.setConf(\"hive.exec.max.dynamic.partitions\", \"10000\")\n",
    "hive_ctx.sql(\"SET spark.sql.parquet.compression.codec=snappy\") \n",
    "\n",
    "hive_ctx.sql(\"use databasename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "Churn_data = hive_ctx.sql(\"Select * from bi_churnanalysistable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 results\n",
    "\n",
    "pd.DataFrame(Churn_data.take(5), columns=Churn_data.columns).transpose() \n",
    "\n",
    "# Same thing but without transpose (less easy to read)\n",
    "\n",
    " # pd.DataFrame(Churn_data.take(5), columns=Churn_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split between training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain the number of rows:\n",
    "Churn_data.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create pandas df\n",
    "Churn_data_Pandas = pd.DataFrame(Churn_data.take(56126), columns=Churn_data.columns) # take(total number of rows)\n",
    "\n",
    "# Split the data\n",
    "Churn_train = Churn_data_Pandas.sample(frac=0.8) \n",
    "Churn_test = Churn_data_Pandas.drop(Churn_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Churn_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cast ischurned as int\n",
    "\n",
    "Churn_train[['ischurned']] = Churn_train[['ischurned']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a look at the different types of data in Churn_data\n",
    "\n",
    "Churn_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only int64 features\n",
    "\n",
    "cols = list(Churn_train.ix[:,3:8] + Churn_train.ix[:,9:17] + Churn_train.ix[:,18:31]) \n",
    "\n",
    "# Select a 10% sample\n",
    "\n",
    "Churn_train[cols].sample(frac=0.1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only the numeric features\n",
    "\n",
    "# numeric_features = [t[0] for t in Churn_train.dtypes if t[1] == 'int64' or t[1] == 'float64'] \n",
    "\n",
    "# sampled_data = Churn_train.select(numeric_features).sample(False, 0.10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Matrix of covariance\n",
    "\n",
    "sampled_data = Churn_train[cols].sample(frac=0.1) \n",
    "\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(12, 12)); \n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: Export to CSV and use R to determine which fields to drop (glm method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If on my computer: \n",
    "# pd.DataFrame.to_csv()\n",
    "# import os\n",
    "# path = 'C:\\Users\\mdeleseleuc\\Documents'\n",
    "# output_filename = 'Churn_train.csv'\n",
    "# Churn_train.to_csv(os.path.join(path,output_filename))\n",
    "\n",
    "# In the cluster:\n",
    "\n",
    "Churn_train_Spark = sqlContext.createDataFrame(Churn_train) # Convert Panda df to Spark df\n",
    "\n",
    "Churn_train_Spark.registerTempTable(\"mytempTable\") # Create temporary table \n",
    "\n",
    "# sqlContext.sql(\"drop table mytable\")\n",
    "sqlContext.sql(\"create table if not exists mytableTrain as select * from mytempTable\") # create table in hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: Drop useless fields by hand (don't do that...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get rid of highly correlated variables\n",
    "\n",
    "Churn_train = Churn_train.drop('lifespan').drop('daysincelastseen') \\\n",
    "             .drop('uniquemapcompleted').drop()\n",
    "\n",
    "Churn_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 3: Feature Selection using tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source 1: http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\"\"\"Source 2: https://kaggle2.blob.core.windows.net/forum-message-attachments/44681/1286/\n",
    "kaggle_forest.py?sv=2012-02-12&se=2016-11-14T22%3A52%3A58Z&sr=b&sp=r&sig=vizTB90DrEljcgvjIGgVVkPowHxAs%2BKO%2BZmqCzf8lms%3D\"\"\"\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import ensemble\n",
    "\n",
    "# feature_cols = [col for col in Churn_train.columns if col in Churn_train[cols]]\n",
    "\n",
    "feature_cols = [col for col in Churn_train.columns if col not in ['s__uid','cohort','monthcohort','avgattemptspermap', \n",
    "                                                                  'successesfailsratio','ischurned']\n",
    "               ]\n",
    "\n",
    "X_train = Churn_train[feature_cols]\n",
    "y = Churn_train['ischurned']\n",
    "\n",
    "X_train.shape              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vérify that columns are the one we wanted\n",
    "\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get rid of NaN values\n",
    "from sklearn.preprocessing import Imputer \n",
    "X_train = Imputer().fit_transform(X_train)  # Imputer = Imputation transformer for completing missing values.\n",
    "\n",
    "clf = ensemble.RandomForestClassifier()\n",
    "clf.fit(X_train, y)\n",
    "\n",
    "# Print the features importance (contribution to total variance)\n",
    "clf.feature_importances_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About fit_transform():\n",
    "\n",
    "Source: http://datascience.stackexchange.com/questions/12321/difference-between-fit-and-fit-transform-in-scikit-learn-models\n",
    "\n",
    "To center the data (make it have zero mean and unit standard error) you subtract the mean, and divide the result by standard deviation.\n",
    "\n",
    "x′=x−μ/σ2\n",
    "\n",
    "You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters μ and σ (values) that you used for centering the training set.\n",
    "\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the new shape\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True) # Meta-transformer for selecting features based on importance weights.\n",
    "X_new = model.transform(X_train) \n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "\n",
    "# Source: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "          #sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "importances = clf.feature_importances_ \n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "  # np.argsort perform an indirect sort along the given axis using the algorithm specified by the kind keyword. \n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "# Informative features:   \n",
    "\n",
    "# Total variance explained (with informative features) = % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reprocessing: Normalize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "\n",
    "http://machinelearningmastery.com/rescaling-data-for-machine-learning-in-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using the Spark MLlib Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees require almost no data preparation (ie normalization) and can handle both categorical and continuous data.\n",
    "\n",
    "To remedy over-fitting and improve prediction accuracy, decision trees can also be limited to a certain depth or complexity, or bundled into ensembles of trees (ie random forests).\n",
    "\n",
    "A decision tree is a predictive model which maps observations (features) about an item to conclusions about the item's label or class. The model is generated using a top-down approach, where the source dataset is split into subsets using a statistical measure, often in the form of the Gini index or information gain via Shannon entropy. This process is applied recursively until a subset contains only samples with the same target class, or is halted by a predefined stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib classifiers and regressors require data sets in a format of rows of type LabeledPoint, which separates row labels and feature lists, and names them accordingly. The custom labelData() function shown below performs the row parsing. We'll pass it the prepared data set (Churn_train) and split it further into training and testing sets. \n",
    "\n",
    "A decision tree classifier model is then generated using the training data, using a maxDepth of 2, to build a \"shallow\" tree. The tree depth can be regarded as an indicator of model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move ischurned to the end \n",
    "\n",
    "Churn_train_map = Churn_train[feature_cols] # columns excluding categorical variables\n",
    "\n",
    "Churn_train_map['ischurned'] = Churn_train['ischurned'] # Add ischurned (at the end)\n",
    "\n",
    "# Did it work?\n",
    "Churn_train_map.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "# LabeledPoint is basically the structure saying which is your target variable (label) and your features vector\n",
    "\n",
    "def labelData(data):\n",
    "    # label: row[end], features: row[0:end-1]\n",
    "    return sqlContext.createDataFrame(data).map(lambda row: LabeledPoint(row[-1], row[:-1]))\n",
    "\n",
    "# sqlContext.createDataFrame(panda_df) = convert to spark_df\n",
    "       \n",
    "training_data, testing_data = labelData(Churn_train_map).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                     categoricalFeaturesInfo = {}, # variables are all continuous\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32) #Max depth = model complexity (see Model Selection)\n",
    " \n",
    "print model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toDebugString() function provides a print of the tree's decision nodes and final prediction outcomes at the end leafs. \n",
    "\n",
    "We can see that features 10 and 16 are used for decision making and should thus be considered as having high predictive power to determine a customer's likeliness to churn. \n",
    "\n",
    "Decision trees are often used for feature selection because they provide an automated mechanism for determining the most important features (those closest to the tree root).\n",
    "\n",
    "N.B: in line with features selection above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sneak peak at the labeled data.\n",
    "\n",
    "training_data.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Feature 13:', Churn_train_map.columns[13]\n",
    "print 'Feature 15:', Churn_train_map.columns[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions of the testing data's churn outcome are made with the model's predict() function and grouped together with the actual churn label of each customer data using getPredictionsLabels().\n",
    "\n",
    "We'll use MLlib's MulticlassMetrics() for the model evaluation, which takes rows of (prediction, label) tuples as input. It provides metrics such as precision, recall, F1 score and confusion matrix, which have been bundled for printing with the custom printMetrics() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def getPredictionsLabels(model, test_data):\n",
    "    predictions = model.predict(test_data.map(lambda r: r.features))\n",
    "    return predictions.zip(test_data.map(lambda r: r.label))\n",
    "\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    print 'Precision of True ', metrics.precision(1)\n",
    "    print 'Precision of False', metrics.precision(0)\n",
    "    print 'Recall of True    ', metrics.recall(1)\n",
    "    print 'Recall of False   ', metrics.recall(0)\n",
    "    print 'F-1 Score         ', metrics.fMeasure()\n",
    "    print 'Confusion Matrix\\n', metrics.confusionMatrix().toArray()\n",
    "\n",
    "predictions_and_labels = getPredictionsLabels(model, testing_data)\n",
    "\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-1 Score = Overall accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall (aka sensitivity) for the Churn=True samples is high, while the recall for the Churn=False examples is relatively low.\n",
    "\n",
    "Perhaps the model's sensitivity bias toward Churn= True samples is due to a skewed distribution of the two types of samples.\n",
    "\n",
    "Let's try grouping the Churn_train DataFrame by the ischurned field and counting the number of instances in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Churn_train.groupby('ischurned')['ischurned'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly 7 times as many True churn samples as False churn samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put the two sample types on the same footing using stratified sampling. The DataFrames sampleBy() function does this when provided with fractions of each sample type to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform into PySpark dataset to be able to use sampleBy\n",
    "\n",
    "Churn_train_strat = sqlContext.createDataFrame(Churn_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're keeping all instances of the Churn=False class, but downsampling the Churn=True class to a fraction of 5672/39229."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stratified_Churn_train = Churn_train_strat.sampleBy('ischurned', fractions={0:1.0, 1: 5672./39229}).cache()\n",
    "\n",
    "stratified_Churn_train.groupby('ischurned').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stratified_Churn_train.describe().toPandas().transpose() # 11,484 rows (reduced the number of true)\n",
    "\n",
    "# Convert dataset to dataframe\n",
    "\n",
    "stratified_Churn_train = pd.DataFrame(stratified_Churn_train.take(11484), columns=stratified_Churn_train.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move ischurned to the end \n",
    "\n",
    "stratified_Churn_train_map = stratified_Churn_train[feature_cols] # columns excluding categorical variables\n",
    "\n",
    "stratified_Churn_train_map['ischurned'] = stratified_Churn_train['ischurned'] # Add ischurned (at the end)\n",
    "\n",
    "# Did it work?\n",
    "Churn_train_map.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the decision tree\n",
    "\n",
    "training_data, testing_data = labelData(stratified_Churn_train_map).randomSplit([0.8, 0.2])\n",
    "\n",
    "model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                     categoricalFeaturesInfo = {}, # variables are all continuous\n",
    "                                     impurity='gini', maxDepth=8, maxBins=32) #Max depth = model complexity\n",
    " \n",
    "print model.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation its accuracy\n",
    " \n",
    "predictions_and_labels = getPredictionsLabels(model, testing_data)\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stratified data was helpful in building a less biased model which will provide more generalized and robust predictions.\n",
    "\n",
    "But: accuracry greatly decreased!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data set at hand, we would like to determine which parameter values of the decision tree produce the best model. We need a systematic approach to quantatively measure the performance of the models and ensure that the results are reliable. \n",
    "\n",
    "This task of model selection is often done using cross validation techniques. \n",
    "\n",
    "A common technique is k-fold cross validation, where the data is randomly split into k partitions. Each partition is used once as the testing data set, while the rest are used for training. Models are then generated using the training sets and evaluated with the testing sets, resulting in k model performance measurements. \n",
    "\n",
    "The average of the performance scores is often taken to be the overall score of the model, given its build parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML package needs data be put in a (label: Double, features: Vector) DataFrame format with correspondingly named fields. The vectorizeData() function below performs this formatting.\n",
    "\n",
    "Next we'll pass the data through a pipeline of two transformers, StringIndexer() and VectorIndexer() which index the label and features fields respectively. Indexing categorical features allows decision trees to treat categorical features appropriately, improving performance. The final element in our pipeline is an estimator (a decision tree classifier) training on the indexed labels and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Churn_train_sel = sqlContext.createDataFrame(Churn_train_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def vectorizeData(data):\n",
    "    return data.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).toDF(['label','features'])\n",
    "\n",
    "vectorized_data = vectorizeData(Churn_train_sel)\n",
    "\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(vectorized_data)\n",
    "\n",
    "# Automatically identify categorical features and index them\n",
    "featureIndexer = VectorIndexer(inputCol='features',\n",
    "                               outputCol='indexedFeatures',\n",
    "                               maxCategories=2).fit(vectorized_data)\n",
    "\n",
    "# Train a DecisionTree model\n",
    "dTree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree])\n",
    "\n",
    "# Search through decision tree's maxDepth parameter for best model\n",
    "paramGrid = ParamGridBuilder().addGrid(dTree.maxDepth, [2,3,4,5,6,7]).build()\n",
    "\n",
    "# Set F-1 score as evaluation metric for best model selection\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',\n",
    "                                              predictionCol='prediction', metricName='f1')    \n",
    "\n",
    "# Set up 3-fold cross validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "CV_model = crossval.fit(vectorized_data)\n",
    "\n",
    "# Fetch best model\n",
    "tree_model = CV_model.bestModel.stages[2]\n",
    "print tree_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the best tree model produced using the cross-validation process is one with a depth of 4. \n",
    "\n",
    "So we can assume that our initial \"shallow\" tree of depth 2 in the previous section was not complex enough, while trees of depth higher than 4 overfit the data and will not perform well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual performance of the model can be determined using the Churn_test set which has not been used for any training or cross-validation activities. We'll transform the test set with the model pipeline, which will map the labels and features according to the same recipe. \n",
    "\n",
    "The evaluator will provide us with the F-1 score of the predictions, and then we'll print them along with their probabilities. Predictions on new, unlabeled customer activity data can also be made using the same pipeline CV_model._transform() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the test data\n",
    "\n",
    "Churn_test_map = Churn_test[feature_cols] # columns excluding categorical variables\n",
    "\n",
    "Churn_test_map['ischurned'] = Churn_test['ischurned'] # Add ischurned (at the end)\n",
    "\n",
    "Churn_test_sel = sqlContext.createDataFrame(Churn_test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "\n",
    "vectorized_test_data = vectorizeData(Churn_test_sel)\n",
    "\n",
    "transformed_data = CV_model.transform(vectorized_test_data)\n",
    "print evaluator.getMetricName(), 'accuracy:', evaluator.evaluate(transformed_data)\n",
    "\n",
    "predictions = transformed_data.select('indexedLabel', 'prediction', 'probability')\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter results by prediction == 1 and sort players by proba desc (to focus on those with highest chance of churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
