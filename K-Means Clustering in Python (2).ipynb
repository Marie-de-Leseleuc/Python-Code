{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://stamfordresearch.com/k-means-clustering-in-python/ \n",
    "\n",
    "Source: https://www.youtube.com/watch?v=Lm1c2U8BmoA (PySpark, ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set Matplotlib inline plotting and load Pandas package\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "data = hive_ctx.sql(\"Select * from bi_temp_kmeanClusteringtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 results\n",
    "\n",
    "df = data.toPandas()\n",
    "\n",
    "df.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe().transpose().tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Nas by mean of column for variable x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df['x'].fillna(df['x'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Nas by 0 for other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only \n",
    "\n",
    "cols = df.columns[(df.dtypes == 'int64') | (df.dtypes == 'float64')] # '|' = or \n",
    "\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[cols].head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New DataFrame without the s__uid \n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix of covariance\n",
    "\n",
    "sampled_data = df[cols].sample(frac=0.1) \n",
    "\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(12, 12)); \n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other type of visuzalization\n",
    "\n",
    "# Source: http://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title('Feature Correlation')\n",
    "    labels= df.columns # not sure\n",
    "    ax1.set_xticklabels(labels,fontsize=6)\n",
    "    ax1.set_yticklabels(labels,fontsize=6)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    cbar = fig.colorbar(cax, ticks=[.25,.3,.35,.4,.45,.5,.55,.6,.65,.70,.75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "    \n",
    "correlation_matrix(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Standardize Features\n",
    " \n",
    " source: http://stackoverflow.com/questions/12525722/normalize-data-in-pandas \n",
    " \n",
    "\"In cluster analysis variables with large values contribute more to the distance calculations. Variables measured on different scales should be standardized prior to clustering, so that the solution is not driven by variables measured on larger scales.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# standardize the data attributes\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(np_scaled)\n",
    "\n",
    "df_normalized.head(5).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set column names back\n",
    "\n",
    "df_normalized.columns = cols\n",
    "\n",
    "df_normalized.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection \n",
    "\n",
    "- RandomForrest\n",
    "- Lasso\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mean clustering using Spark ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_normalized.dtypes\n",
    "\n",
    "type(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_normalized = sqlContext.createDataFrame(df_normalized)\n",
    "\n",
    "'''\n",
    "vectorAssembler = VectorAssembler(inputCols= df_normalized.columns,\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df = vectorAssembler.transform(df_normalized)\n",
    "'''\n",
    "\n",
    "rdd = df_normalized.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "\n",
    "clusters = KMeans.train(rdd, 5, maxIterations=10, initializationMode=\"random\") # 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class RowIterator(TransformerMixin):\n",
    "    \"\"\" Prepare dataframe for DictVectorizer \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (row for _, row in X.iterrows())\n",
    "\n",
    "\n",
    "vectorizer = make_pipeline(RowIterator(), DictVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the sum of Squared Error:\n",
    "\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Within-cluster sum of squares\n",
    "\n",
    "WSSE = (rdd.map(lambda point: error(point))\n",
    "                   .reduce(lambda x,y: x+y))\n",
    "\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try with a range of number of clusters\n",
    "\n",
    "for l in range(1,6):\n",
    "    clusters = KMeans.train(rdd, l, maxIterations = 100, runs = 100, initializationMode = 'random')\n",
    "    WSSSE = (rdd.map(lambda point: error(point))\n",
    "                .reduce(lambda x,y: x+y))\n",
    "    print(\"With \" + str(l) + ' clusters: Within Set Sum of Squared Error =' + str(WSSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-mean clustering using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MasterClass: https://github.com/Marie-de-Leseleuc/Python-Code/blob/master/exo%2B1%20(2).ipynb \n",
    "\n",
    "1. https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials \n",
    "\n",
    "2. https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/\n",
    "\n",
    "3. http://mnemstudio.org/clustering-k-means-example-1.htm\n",
    "\n",
    "4. https://www.dataquest.io/blog/k-means-clustering-us-senators/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: K-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: http://stackoverflow.com/questions/28017091/will-pandas-dataframe-object-work-with-sklearn-kmeans-clustering \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "dataset = df_normalized\n",
    "\n",
    "# Convert DataFrame to matrix\n",
    "mat = dataset.as_matrix()\n",
    "\n",
    "# Using sklearn\n",
    "km = KMeans(n_clusters=5)\n",
    "km.fit(mat)\n",
    "\n",
    "# Get cluster assignment labels\n",
    "labels = km.labels_\n",
    "\n",
    "# Format results as a DataFrame\n",
    " # results = pd.DataFrame([dataset.index,labels]).T # return a df with the cluster corresponding to each index\n",
    "\n",
    "results = pd.DataFrame(data=labels, columns=['cluster'], index=dataset.index) # better way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add cluster number to each observation \n",
    "\n",
    "print(len(dataset), len(results))\n",
    "\n",
    "dataset_f = dataset\n",
    "\n",
    "dataset_f['cluster'] = results # Add cluster number to df\n",
    "\n",
    "dataset_f['id'] = data.toPandas()['id'] # add s__uid to df\n",
    "\n",
    "dataset_f[['cluster', 'id']].head(5) # produce cluster by player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the clusters\n",
    "\n",
    "label = dataset_f.cluster.unique()\n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    print(dataset_f[dataset_f[\"cluster\"]==label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw histograms\n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    dataset_f[dataset_f[\"cluster\"]==label].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the cluster per variable\n",
    "\n",
    "dataset_f.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Personal exercice: denormalize the data\n",
    "\n",
    "dataset.drop('id', axis=1, inplace=True)\n",
    "\n",
    "dataset.drop('cluster', axis=1, inplace=True)\n",
    "\n",
    "le = min_max_scaler.inverse_transform(dataset_f) # unscale the data\n",
    "\n",
    "le = pd.DataFrame(le)\n",
    "\n",
    "le.columns = cols\n",
    "\n",
    "result = pd.DataFrame(data=labels, columns=['cluster'], index=le.index) \n",
    "\n",
    "dataset_f = le\n",
    "\n",
    "dataset_f['cluster'] = result # Add cluster number to df\n",
    "\n",
    "dataset_f['id'] = data.toPandas()['id'] # add s__uid to df\n",
    "\n",
    "dataset_f[['cluster', 'id']].head(5) # produce cluster by player\n",
    "\n",
    "dataset_f.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_f.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: K-mean ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: http://stackoverflow.com/questions/34958994/how-to-use-scikit-kmeans-when-i-have-a-dataframe\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "dataset = df_normalized\n",
    "\n",
    "sample_df_train, sample_df_test = sklearn.cross_validation.train_test_split(dataset, train_size=0.6)\n",
    "\n",
    "cluster = sklearn.cluster.KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=300, tol=0.0001, \n",
    "                                 precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1)\n",
    "\n",
    "cluster.fit(sample_df_train)\n",
    "\n",
    "result = cluster.predict(sample_df_train)\n",
    "\n",
    "results = cluster.predict(sample_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not sure it works....\n",
    "\n",
    "labels = cluster.labels_\n",
    "\n",
    "result = pd.DataFrame([sample_df_train.index,labels]).T \n",
    "\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the dataframe and the cluster analysis result\n",
    "\n",
    "# Create a unique identifier using the index\n",
    "sample_df_train.reset_index(level=0, inplace=True) \n",
    "\n",
    "# create a list with the index for each observation\n",
    "cluslist=list(sample_df_train['index'])   \n",
    "\n",
    "# create a lit with the cluster label for each observation\n",
    "labels=list(cluster.labels_)\n",
    "\n",
    "# create a dictionnary with the two lists\n",
    "newlist=dict(zip(cluslist, labels))\n",
    "\n",
    "# create a dataframe from the dictionnary \n",
    "newclus= pd.DataFrame.from_dict(newlist, orient='index')\n",
    "\n",
    "# create a column with the cluster labels\n",
    "newclus.columns = ['cluster']\n",
    "\n",
    "# Create a unique identifier using the index\n",
    "newclus.reset_index(level=0, inplace=True)\n",
    "\n",
    "# merge the dataframes\n",
    "merged_train=pd.merge(sample_df_train, newclus, on='index')\n",
    "\n",
    "merged_train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the clusters\n",
    "\n",
    "Listlabels = merged_train.cluster.unique()\n",
    "\n",
    "labels = [x for x in labels if str(x) != 'nan'] # get rid of Nan\n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    print(merged_train[merged_train[\"cluster\"]==label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_train.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare clusters per variable\n",
    "\n",
    "merged_train.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "\n",
    "labels = range(0,1) # keep only the first \n",
    "\n",
    "for label in set(labels):\n",
    "    print(\"Label:\",label)\n",
    "    merged_train[merged_train[\"cluster\"]==label].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: K-mean + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Source: \n",
    "#https://www.coursera.org/learn/machine-learning-data-analysis/lecture/Ebb2M/running-a-k-means-cluster-analysis-in-python-pt-1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "dataset = df_normalized.drop('y', 1) # Will use variable y to assess the clusters i.e. get rid of it here. \n",
    "\n",
    "# split data into train and test sets\n",
    "clus_train, clus_test = train_test_split(dataset, test_size=.3, random_state=123)\n",
    "\n",
    "# k-means cluster analysis for 1-9 clusters                                                           \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,10)\n",
    "meandist=[] #store distance values from the cluster centroids\n",
    "\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)  # specify number of clusters to use for the analysis\n",
    "    model.fit(clus_train)  # cluster analysis\n",
    "    clusassign=model.predict(clus_train) # assign cluster number to each observation based on the cluster analysis \n",
    "                                           # (i.e. assigned to closer cluster)\n",
    "    meandist.append(sum(np.min(cdist(clus_train, model.cluster_centers_, 'euclidean'), axis=1)) # computes the average of \n",
    "                                            # the sum of the distances between each observation and a cluster centroids\n",
    "    / clus_train.shape[0]) # divide the sum of distances by the nb of observation in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "\n",
    "plt.plot(clusters, meandist) # cluster 1 to 9 vs. avg distance that has just been calculated\n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the decrease in the avg minimun distance of the observations from the cluster centroids for each of the cluster solutions.\n",
    "\n",
    "We can see that the avg distance decreases as the number of clusters increases. Since the goal of cluster analysis is to minimize the distance between observations and their assinged clusters, you want to chose the fewest amount of clusters that provide the low avg distance.\n",
    "\n",
    "What we're looking for in this plot is a bend in the elbow that kind of shows where the average distance value might be leveling off such that adding more clusters doesn't decrease the average distance as much. \n",
    "\n",
    "Very subjective. Need to further examine the cluster solutions to see whether they do not overlap, wheter the patterns of means on the clustering variables are unique and meaningful and wether there are signifiant differences between the clusters on the external validation variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpret 5 cluster solution (rerun the cluster analysis, this time asking for 5 clusters)\n",
    "model3=KMeans(n_clusters=5) # create object 3 which contains the result from the cluster analaysis with 5 clusters\n",
    "model3.fit(clus_train)      # fit the model\n",
    "clusassign=model3.predict(clus_train)  # create an object that has the cluster assignment based on the 5 clusters model\n",
    "\n",
    "# plot clusters (try to see if the clusters overlap with each other in the p-dimension space)\n",
    "\n",
    "# 42 variables i.e. 42 dimensions, which would be impossible to vizualise. So will use canonical discriminant analysis \n",
    "# which is a data reduction technique that creates a smaller number of variables that are linear combinaison of \n",
    "# the 42 clustering variables.\n",
    "\n",
    "# Canonical variables are ordered by proportion of variance accounted for.\n",
    "# Majority of variance is accounted (i.e.accounts for as much of the variability in the data as possible) by first few \n",
    "# canonical variables and those are the ones that we can plot. \n",
    "# variability: dispersion: the extent to which a distribution is stretched or squeezed.\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "pca_2 = PCA(2) # returns the two first canonical variable\n",
    "\n",
    "plot_columns = pca_2.fit_transform(clus_train) \n",
    "    # create a matrix that include the 2 canonical var. estimated by the PCA\n",
    "    \n",
    "    # PCA_2.fit asks Python to fit the canonical discriminate analysis that we specified with the PCA command,\n",
    "    # and the _transform applies the canonical discriminate analysis to the clus_train data set to calculate \n",
    "    # the canonical variables\n",
    "    \n",
    "plt.scatter(x=plot_columns[:,0], y=plot_columns[:,1], c=model3.labels_,) # plot first and second canonical variables \n",
    "                                                                         # (first column & second column from plot_column)\n",
    "        #  c=model3.labels_ tells python to color code the points for each of the clusters\n",
    "    \n",
    "plt.xlabel('Canonical variable 1')\n",
    "plt.ylabel('Canonical variable 2')\n",
    "plt.title('Scatterplot of Canonical Variables for 5 Clusters')\n",
    "plt.show()\n",
    "\n",
    "# .transform() explaination: http://stackoverflow.com/questions/27517425/apply-vs-transform-on-a-group-object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A canonical variate is a new variable (variate) formed by making a linear combination of two or more variates (variables) from a data set.\n",
    "\n",
    "3 clusters with a good separation. Possible correlation between two others. 4 clusters better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BEGIN multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\"\n",
    "\n",
    "# Let's look at the pattern of means on the clustering variables for each cluster to see whether they are distinct and meaningful\n",
    "# To do this, we have to link the cluster assigment variables back to its corresponding observation in the clus_train dataset.\n",
    "\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster training data to merge with the cluster assignment variable\n",
    "clus_train.reset_index(level=0, inplace=True) # use the index to create a new variable labeled 'index' \n",
    "                                              # that we can use as a unique identifier.\n",
    "  \n",
    "    # level=0 tells Python to only remove the given levels from the index\n",
    "    # inplace=True, tells Python to add the new column to the existing clus_train dataset.\n",
    "\n",
    "# create a list that has the new index variable\n",
    "cluslist=list(clus_train['index'])   \n",
    "\n",
    "# create a list of cluster assignments \n",
    "labels=list(model3.labels_)\n",
    "\n",
    "    # This will be combined with the cluster assignment variable,\n",
    "    # so that we can merge the two datasets together by each observation's unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine index variable list with cluster assignment list into a dictionary\n",
    "newlist=dict(zip(cluslist, labels)) # zip is used to combided list \n",
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select specific rows  \n",
    "interesting_keys = range(0,6)\n",
    "subdict = {x: newlist[x] for x in interesting_keys if x in newlist}\n",
    "subdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert newlist dictionary to a dataframe\n",
    "newclus= pd.DataFrame.from_dict(newlist, orient='index')\n",
    "newclus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename the cluster assignment column\n",
    "newclus.columns = ['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newclus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now do the same for the cluster assignment variable\n",
    "# create a unique identifier variable from the index for the \n",
    "# cluster assignment dataframe \n",
    "# to merge with cluster training data\n",
    "\n",
    "newclus.reset_index(level=0, inplace=True)\n",
    "\n",
    "# merge the cluster assignment dataframe with the cluster training variable dataframe\n",
    "# by the index variable\n",
    "\n",
    "merged_train=pd.merge(clus_train, newclus, on='index')\n",
    "merged_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cluster frequencies\n",
    "merged_train.cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "END multiple steps to merge cluster assignment with clustering variables to examine\n",
    "cluster variable means by cluster\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FINALLY calculate clustering variable means by cluster\n",
    "clustergrp = merged_train.groupby('cluster').mean() \n",
    "print (\"Clustering variable means by cluster\")\n",
    "print(clustergrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the variables are standardise to be on the same scale with an overall sample mean of zero and a standard deviation of 1\n",
    "\n",
    "Cluster 3, has the highest likelihood of connections to the game (and generaly of most variables). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the clusters differ on total playtime in the game: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validate clusters in training data by examining cluster differences in y using ANOVA\n",
    "# first have to merge y with clustering variables and cluster assignment data \n",
    "\n",
    "# Extract data from original dataset\n",
    "tpt_data=df['y']\n",
    "\n",
    "# split totalplaytime data into train and test sets\n",
    "tpt_train, tpt_test = train_test_split(tpt_data, test_size=.3, random_state=123)\n",
    "\n",
    "tpt_train1=pd.DataFrame(tpt_train)\n",
    "\n",
    "tpt_train1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge the training observations in the dataset that includes the cluster assignment variable \n",
    "# with the training observations in the dataset that includes the clustering variables. \n",
    "\n",
    "tpt_train1.reset_index(level=0, inplace=True)\n",
    "\n",
    "merged_train_all=pd.merge(tpt_train1, merged_train, on='index')\n",
    "\n",
    "merged_train_all.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge total play time and cluster assignment data\n",
    "\n",
    "sub1 = merged_train_all[['y', 'cluster']].dropna()\n",
    "\n",
    "sub1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is there significant differences between clusters on the quantitative totalplaytime variable?\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as multi \n",
    "\n",
    "# We use the ols function to test the analysis of variance. \n",
    "# The formula specifies the model, with y as the response variable and cluster, as the explanatory variable.\n",
    "\n",
    "#The capital C tells Python that the cluster assignment variable is categorical\n",
    "\n",
    "tptmod = smf.ols(formula='y ~ C(cluster)', data=sub1).fit()\n",
    "print (tptmod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also print the mean y for each cluster using the groupby function\n",
    "\n",
    "print ('means for y by cluster')\n",
    "m1= sub1.groupby('cluster').mean()\n",
    "print (m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And the standard deviation\n",
    "\n",
    "print ('standard deviations for y by cluster')\n",
    "m2= sub1.groupby('cluster').std()\n",
    "print (m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Then, because our categorical cluster variable has 5 categories, \n",
    "# we will request a tukey test to evaluate post hot comparisons between the clusters \n",
    "\n",
    "mc1 = multi.MultiComparison(sub1['y'], sub1['cluster'])\n",
    "res1 = mc1.tukeyhsd()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of variance summary table indicates that the clusters do not differ significantly on total play time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Masterclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source: http://stackoverflow.com/questions/34175462/dendrogram-using-pandas-and-scipy\n",
    "\n",
    "from scipy.cluster import hierarchy as hc\n",
    "\n",
    "dataframe = merged_train\n",
    "corr = 1 - dataframe.corr() \n",
    "\n",
    "corr_condensed = hc.distance.squareform(corr) # convert to condensed\n",
    "z = hc.linkage(corr_condensed, method='average')\n",
    "dendrogram = hc.dendrogram(z, labels=corr.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n",
    "\n",
    "- horizontal lines are cluster merges\n",
    "- vertical lines tell you which clusters/labels were part of merge forming that new cluster\n",
    "- heights of the horizontal lines tell you about the distance that needed to be \"bridged\" to form the new cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical Clustering (see Benjamin's document)\n",
    "\n",
    "# Note: too much data? Seem to return an overflowing error\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(merged_train.as_matrix(), 'ward') \n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
